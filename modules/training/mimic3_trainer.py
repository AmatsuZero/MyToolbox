#!/usr/bin/env python3
"""
Mimic3 è®­ç»ƒæ¥å£å°è£…æ¨¡å—

å°è£… Mimic3 TTS è®­ç»ƒ APIï¼Œæä¾›è®­ç»ƒå‚æ•°é…ç½®ã€æ¨¡å‹è¾“å‡ºç®¡ç†å’Œè®­ç»ƒè¿›åº¦å›è°ƒåŠŸèƒ½ã€‚
"""

import os
import json
import shutil
import subprocess
import time
from pathlib import Path
from dataclasses import dataclass, field
from typing import Callable, Optional, List, Dict, Any
from enum import Enum

from modules.training.train_config import TrainConfig
from modules.training.mimic3_checker import ensure_mimic3_available


class TrainingStatus(Enum):
    """è®­ç»ƒçŠ¶æ€æšä¸¾"""
    IDLE = "idle"               # ç©ºé—²çŠ¶æ€
    PREPARING = "preparing"     # å‡†å¤‡ä¸­
    TRAINING = "training"       # è®­ç»ƒä¸­
    PAUSED = "paused"          # å·²æš‚åœ
    COMPLETED = "completed"     # å·²å®Œæˆ
    FAILED = "failed"          # å¤±è´¥
    CANCELLED = "cancelled"     # å·²å–æ¶ˆ


@dataclass
class TrainingProgress:
    """è®­ç»ƒè¿›åº¦ä¿¡æ¯"""
    current_epoch: int = 0          # å½“å‰ epoch
    total_epochs: int = 0           # æ€» epoch æ•°
    current_step: int = 0           # å½“å‰æ­¥æ•°
    total_steps: int = 0            # æ€»æ­¥æ•°
    loss: float = 0.0               # å½“å‰æŸå¤±å€¼
    learning_rate: float = 0.0      # å½“å‰å­¦ä¹ ç‡
    elapsed_time: float = 0.0       # å·²ç”¨æ—¶é—´ï¼ˆç§’ï¼‰
    estimated_remaining: float = 0.0 # é¢„ä¼°å‰©ä½™æ—¶é—´ï¼ˆç§’ï¼‰
    status: TrainingStatus = TrainingStatus.IDLE
    message: str = ""               # çŠ¶æ€æ¶ˆæ¯
    
    @property
    def progress_percentage(self) -> float:
        """è®¡ç®—è®­ç»ƒè¿›åº¦ç™¾åˆ†æ¯”"""
        if self.total_epochs == 0:
            return 0.0
        return (self.current_epoch / self.total_epochs) * 100
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "current_epoch": self.current_epoch,
            "total_epochs": self.total_epochs,
            "current_step": self.current_step,
            "total_steps": self.total_steps,
            "loss": self.loss,
            "learning_rate": self.learning_rate,
            "elapsed_time": self.elapsed_time,
            "estimated_remaining": self.estimated_remaining,
            "progress_percentage": self.progress_percentage,
            "status": self.status.value,
            "message": self.message
        }


@dataclass
class TrainingResult:
    """è®­ç»ƒç»“æœä¿¡æ¯"""
    success: bool = False           # æ˜¯å¦æˆåŠŸ
    model_path: Optional[Path] = None  # æ¨¡å‹æ–‡ä»¶è·¯å¾„
    total_epochs: int = 0           # è®­ç»ƒçš„æ€» epoch æ•°
    final_loss: float = 0.0         # æœ€ç»ˆæŸå¤±å€¼
    training_time: float = 0.0      # æ€»è®­ç»ƒæ—¶é—´ï¼ˆç§’ï¼‰
    model_size: int = 0             # æ¨¡å‹æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    error_message: str = ""         # é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰
    checkpoint_path: Optional[Path] = None  # æ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ï¼‰
    logs: List[str] = field(default_factory=list)  # è®­ç»ƒæ—¥å¿—
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "success": self.success,
            "model_path": str(self.model_path) if self.model_path else None,
            "total_epochs": self.total_epochs,
            "final_loss": self.final_loss,
            "training_time": self.training_time,
            "model_size": self.model_size,
            "error_message": self.error_message,
            "checkpoint_path": str(self.checkpoint_path) if self.checkpoint_path else None
        }


# è¿›åº¦å›è°ƒå‡½æ•°ç±»å‹
ProgressCallback = Callable[[TrainingProgress], None]


class Mimic3Trainer:
    """
    Mimic3 è®­ç»ƒå™¨
    
    å°è£… Mimic3 TTS æ¨¡å‹è®­ç»ƒåŠŸèƒ½ï¼Œæ”¯æŒï¼š
    - è®­ç»ƒå‚æ•°é…ç½®
    - è®­ç»ƒè¿›åº¦ç›‘æ§
    - æ–­ç‚¹ç»­è®­
    - æ¨¡å‹è¾“å‡ºç®¡ç†
    """
    
    # Mimic3 è®­ç»ƒè„šæœ¬è·¯å¾„ï¼ˆæ ¹æ®å®é™…å®‰è£…ä½ç½®è°ƒæ•´ï¼‰
    MIMIC3_TRAIN_SCRIPT = "mimic3-train"
    
    def __init__(self, config: TrainConfig, verbose: bool = False):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config: è®­ç»ƒé…ç½®
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
        """
        self.config = config
        self.verbose = verbose
        self._progress = TrainingProgress()
        self._callbacks: List[ProgressCallback] = []
        self._process: Optional[subprocess.Popen] = None
        self._start_time: float = 0
        self._should_stop: bool = False
        self._logs: List[str] = []
    
    def add_progress_callback(self, callback: ProgressCallback) -> None:
        """
        æ·»åŠ è¿›åº¦å›è°ƒå‡½æ•°
        
        Args:
            callback: å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ TrainingProgress å‚æ•°
        """
        self._callbacks.append(callback)
    
    def remove_progress_callback(self, callback: ProgressCallback) -> None:
        """
        ç§»é™¤è¿›åº¦å›è°ƒå‡½æ•°
        
        Args:
            callback: è¦ç§»é™¤çš„å›è°ƒå‡½æ•°
        """
        if callback in self._callbacks:
            self._callbacks.remove(callback)
    
    def _notify_progress(self) -> None:
        """é€šçŸ¥æ‰€æœ‰å›è°ƒå‡½æ•°å½“å‰è¿›åº¦"""
        for callback in self._callbacks:
            try:
                callback(self._progress)
            except Exception as e:
                self._log(f"è¿›åº¦å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")
    
    def _log(self, message: str) -> None:
        """
        è®°å½•æ—¥å¿—
        
        Args:
            message: æ—¥å¿—æ¶ˆæ¯
        """
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        self._logs.append(log_entry)
        if self.verbose:
            print(log_entry)
    
    def _update_progress(self, **kwargs) -> None:
        """
        æ›´æ–°è®­ç»ƒè¿›åº¦
        
        Args:
            **kwargs: è¦æ›´æ–°çš„è¿›åº¦å±æ€§
        """
        for key, value in kwargs.items():
            if hasattr(self._progress, key):
                setattr(self._progress, key, value)
        
        # è®¡ç®—å·²ç”¨æ—¶é—´
        if self._start_time > 0:
            self._progress.elapsed_time = time.time() - self._start_time
            
            # ä¼°ç®—å‰©ä½™æ—¶é—´
            if self._progress.current_epoch > 0:
                avg_time_per_epoch = self._progress.elapsed_time / self._progress.current_epoch
                remaining_epochs = self._progress.total_epochs - self._progress.current_epoch
                self._progress.estimated_remaining = avg_time_per_epoch * remaining_epochs
        
        self._notify_progress()
    
    def _prepare_training_directory(self, dataset_path: Path) -> Path:
        """
        å‡†å¤‡è®­ç»ƒç›®å½•ç»“æ„
        
        Args:
            dataset_path: æ•°æ®é›†è·¯å¾„
            
        Returns:
            è®­ç»ƒå·¥ä½œç›®å½•è·¯å¾„
        """
        self._log("å‡†å¤‡è®­ç»ƒç›®å½•...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = self.config.output_path
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºè®­ç»ƒå·¥ä½œç›®å½•
        work_dir = output_dir / "training_workspace"
        work_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
        checkpoint_dir = work_dir / "checkpoints"
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = work_dir / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        
        self._log(f"è®­ç»ƒå·¥ä½œç›®å½•: {work_dir}")
        return work_dir
    
    def _generate_training_config(self, dataset_path: Path, work_dir: Path) -> Path:
        """
        ç”Ÿæˆ Mimic3 è®­ç»ƒé…ç½®æ–‡ä»¶
        
        Args:
            dataset_path: æ•°æ®é›†è·¯å¾„
            work_dir: å·¥ä½œç›®å½•
            
        Returns:
            é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self._log("ç”Ÿæˆè®­ç»ƒé…ç½®æ–‡ä»¶...")
        
        # Mimic3 è®­ç»ƒé…ç½®
        training_config = {
            "audio": {
                "sample_rate": self.config.sample_rate,
                "num_mels": 80,
                "fft_size": 1024,
                "hop_size": 256,
                "win_size": 1024,
                "fmin": 0,
                "fmax": 8000
            },
            "model": {
                "type": "vits",
                "hidden_channels": 192,
                "inter_channels": 192,
                "filter_channels": 768,
                "n_heads": 2,
                "n_layers": 6,
                "kernel_size": 3,
                "p_dropout": 0.1
            },
            "training": {
                "epochs": self.config.epochs,
                "batch_size": self.config.batch_size,
                "learning_rate": 0.0002,
                "lr_decay": 0.999875,
                "seed": 1234,
                "fp16_run": False,
                "grad_clip_thresh": 1.0,
                "checkpoint_interval": 1000,
                "validation_interval": 1000,
                "log_interval": 100
            },
            "data": {
                "dataset_path": str(dataset_path),
                "speaker_name": self.config.speaker_name,
                "text_cleaners": ["basic_cleaners"]
            },
            "output": {
                "model_dir": str(self.config.output_path / "model"),
                "checkpoint_dir": str(work_dir / "checkpoints"),
                "log_dir": str(work_dir / "logs")
            }
        }
        
        # å†™å…¥é…ç½®æ–‡ä»¶
        config_path = work_dir / "training_config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(training_config, f, indent=2, ensure_ascii=False)
        
        self._log(f"é…ç½®æ–‡ä»¶å·²ä¿å­˜: {config_path}")
        return config_path
    
    def _build_training_command(self, config_path: Path) -> List[str]:
        """
        æ„å»ºè®­ç»ƒå‘½ä»¤
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            
        Returns:
            å‘½ä»¤å‚æ•°åˆ—è¡¨
        """
        cmd = [
            self.MIMIC3_TRAIN_SCRIPT,
            "--config", str(config_path),
            "--epochs", str(self.config.epochs),
            "--batch-size", str(self.config.batch_size),
            "--sample-rate", str(self.config.sample_rate),
            "--output", str(self.config.output_path / "model"),
        ]
        
        if self.verbose:
            cmd.append("--verbose")
        
        return cmd
    
    def _parse_training_output(self, line: str) -> None:
        """
        è§£æè®­ç»ƒè¾“å‡ºï¼Œæ›´æ–°è¿›åº¦
        
        Args:
            line: è¾“å‡ºè¡Œ
        """
        line = line.strip()
        if not line:
            return
        
        self._log(line)
        
        # è§£æ epoch ä¿¡æ¯
        # å‡è®¾è¾“å‡ºæ ¼å¼ç±»ä¼¼: "Epoch 1/100, Step 500/10000, Loss: 0.5432, LR: 0.0002"
        try:
            if "Epoch" in line:
                parts = line.split(",")
                for part in parts:
                    part = part.strip()
                    if "Epoch" in part:
                        epoch_info = part.replace("Epoch", "").strip()
                        if "/" in epoch_info:
                            current, total = epoch_info.split("/")
                            self._update_progress(
                                current_epoch=int(current.strip()),
                                total_epochs=int(total.strip())
                            )
                    elif "Step" in part:
                        step_info = part.replace("Step", "").strip()
                        if "/" in step_info:
                            current, total = step_info.split("/")
                            self._update_progress(
                                current_step=int(current.strip()),
                                total_steps=int(total.strip())
                            )
                    elif "Loss" in part:
                        loss_value = part.replace("Loss:", "").strip()
                        self._update_progress(loss=float(loss_value))
                    elif "LR" in part:
                        lr_value = part.replace("LR:", "").strip()
                        self._update_progress(learning_rate=float(lr_value))
        except (ValueError, IndexError) as e:
            # è§£æå¤±è´¥ï¼Œå¿½ç•¥
            pass
    
    def train(self, dataset_path: Path, resume_from: Optional[Path] = None) -> TrainingResult:
        """
        æ‰§è¡Œè®­ç»ƒ
        
        Args:
            dataset_path: æ•°æ®é›†è·¯å¾„ï¼ˆç”± AudioAligner ç”Ÿæˆçš„å¯¹é½æ•°æ®ï¼‰
            resume_from: æ–­ç‚¹ç»­è®­çš„æ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            è®­ç»ƒç»“æœ
        """
        result = TrainingResult()
        self._logs = []
        self._should_stop = False
        
        try:
            # æ›´æ–°çŠ¶æ€ä¸ºå‡†å¤‡ä¸­
            self._update_progress(
                status=TrainingStatus.PREPARING,
                message="æ­£åœ¨å‡†å¤‡è®­ç»ƒç¯å¢ƒ..."
            )
            
            # å‡†å¤‡è®­ç»ƒç›®å½•
            work_dir = self._prepare_training_directory(dataset_path)
            
            # ç”Ÿæˆè®­ç»ƒé…ç½®
            config_path = self._generate_training_config(dataset_path, work_dir)
            
            # æ„å»ºè®­ç»ƒå‘½ä»¤
            cmd = self._build_training_command(config_path)
            
            # å¦‚æœæ˜¯æ–­ç‚¹ç»­è®­ï¼Œæ·»åŠ æ¢å¤å‚æ•°
            if resume_from and resume_from.exists():
                cmd.extend(["--resume", str(resume_from)])
                self._log(f"ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {resume_from}")
            
            self._log(f"è®­ç»ƒå‘½ä»¤: {' '.join(cmd)}")
            
            # æ›´æ–°çŠ¶æ€ä¸ºè®­ç»ƒä¸­
            self._update_progress(
                status=TrainingStatus.TRAINING,
                message="è®­ç»ƒè¿›è¡Œä¸­...",
                total_epochs=self.config.epochs
            )
            
            # è®°å½•å¼€å§‹æ—¶é—´
            self._start_time = time.time()
            
            # å¯åŠ¨è®­ç»ƒè¿›ç¨‹
            self._log("å¯åŠ¨è®­ç»ƒè¿›ç¨‹...")
            
            # æ³¨æ„ï¼šç”±äº Mimic3 å¯èƒ½æœªå®‰è£…ï¼Œè¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿè®­ç»ƒ
            # å®é™…ä½¿ç”¨æ—¶åº”æ›¿æ¢ä¸ºçœŸå®çš„ subprocess è°ƒç”¨
            if self._is_mimic3_available():
                result = self._run_real_training(cmd, work_dir)
            else:
                self._log("è­¦å‘Š: Mimic3 è®­ç»ƒè„šæœ¬æœªæ‰¾åˆ°ï¼Œæ‰§è¡Œæ¨¡æ‹Ÿè®­ç»ƒ")
                result = self._run_simulated_training(work_dir)
            
        except Exception as e:
            self._log(f"è®­ç»ƒè¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
            self._update_progress(
                status=TrainingStatus.FAILED,
                message=f"è®­ç»ƒå¤±è´¥: {e}"
            )
            result.success = False
            result.error_message = str(e)
        
        result.logs = self._logs.copy()
        return result
    
    def _is_mimic3_available(self) -> bool:
        """æ£€æŸ¥ Mimic3 è®­ç»ƒè„šæœ¬æ˜¯å¦å¯ç”¨
        
        ä¸“é—¨æ£€æŸ¥ mimic3-train å‘½ä»¤è¡Œå·¥å…·æ˜¯å¦åœ¨ PATH ä¸­ï¼Œ
        è€Œä¸ä»…ä»…æ˜¯æ£€æŸ¥ Python åŒ…æ˜¯å¦å®‰è£…ã€‚
        """
        # æ£€æŸ¥ mimic3-train å‘½ä»¤æ˜¯å¦å­˜åœ¨äº PATH ä¸­
        if shutil.which("mimic3-train"):
            return True
        
        # å‘½ä»¤è¡Œå·¥å…·ä¸å¯ç”¨ï¼Œæ‰“å°è­¦å‘Šä¿¡æ¯
        self._log("=" * 60)
        self._log("âš ï¸ è­¦å‘Š: mimic3-train å‘½ä»¤æœªæ‰¾åˆ°ï¼")
        self._log("=" * 60)
        self._log("å°†ä½¿ç”¨ã€æ¨¡æ‹Ÿè®­ç»ƒæ¨¡å¼ã€‘ï¼Œæ­¤æ¨¡å¼ä¸ä¼šç”Ÿæˆæœ‰æ•ˆçš„ ONNX æ¨¡å‹ã€‚")
        self._log("æ¨¡æ‹Ÿè®­ç»ƒä»…ç”¨äºæµ‹è¯•è®­ç»ƒæµç¨‹ï¼Œç”Ÿæˆçš„æ–‡ä»¶æ— æ³•ç”¨äºè¯­éŸ³åˆæˆã€‚")
        self._log("")
        self._log("å¦‚éœ€ç”ŸæˆçœŸæ­£å¯ç”¨çš„ ONNX æ¨¡å‹ï¼Œè¯·å…ˆå®‰è£… mimic3-train:")
        self._log("  pip install mimic3-train")
        self._log("æˆ–å‚è€ƒå®˜æ–¹æ–‡æ¡£: https://github.com/MycroftAI/mimic3")
        self._log("=" * 60)
        return False
    
    def _run_real_training(self, cmd: List[str], work_dir: Path) -> TrainingResult:
        """
        æ‰§è¡ŒçœŸå®çš„ Mimic3 è®­ç»ƒ
        
        Args:
            cmd: è®­ç»ƒå‘½ä»¤
            work_dir: å·¥ä½œç›®å½•
            
        Returns:
            è®­ç»ƒç»“æœ
        """
        result = TrainingResult()
        
        try:
            self._process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
                cwd=work_dir
            )
            
            # è¯»å–è¾“å‡ºå¹¶è§£æè¿›åº¦
            for line in iter(self._process.stdout.readline, ''):
                if self._should_stop:
                    self._process.terminate()
                    self._update_progress(
                        status=TrainingStatus.CANCELLED,
                        message="è®­ç»ƒå·²å–æ¶ˆ"
                    )
                    result.error_message = "è®­ç»ƒè¢«ç”¨æˆ·å–æ¶ˆ"
                    return result
                
                self._parse_training_output(line)
            
            # ç­‰å¾…è¿›ç¨‹ç»“æŸ
            return_code = self._process.wait()
            
            if return_code == 0:
                result.success = True
                self._update_progress(
                    status=TrainingStatus.COMPLETED,
                    message="è®­ç»ƒå®Œæˆ"
                )
            else:
                result.success = False
                result.error_message = f"è®­ç»ƒè¿›ç¨‹é€€å‡ºç : {return_code}"
                self._update_progress(
                    status=TrainingStatus.FAILED,
                    message=f"è®­ç»ƒå¤±è´¥ï¼Œé€€å‡ºç : {return_code}"
                )
            
        except Exception as e:
            result.success = False
            result.error_message = str(e)
            self._update_progress(
                status=TrainingStatus.FAILED,
                message=f"è®­ç»ƒå¤±è´¥: {e}"
            )
        
        finally:
            self._process = None
        
        # æ”¶é›†è®­ç»ƒç»“æœ
        result.training_time = time.time() - self._start_time
        result.total_epochs = self._progress.current_epoch
        result.final_loss = self._progress.loss
        
        # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
        model_dir = self.config.output_path / "model"
        if model_dir.exists():
            model_files = list(model_dir.glob("*.onnx")) + list(model_dir.glob("*.pt"))
            if model_files:
                result.model_path = model_files[0]
                result.model_size = result.model_path.stat().st_size
        
        # æŸ¥æ‰¾æ£€æŸ¥ç‚¹
        checkpoint_dir = work_dir / "checkpoints"
        if checkpoint_dir.exists():
            checkpoints = sorted(checkpoint_dir.glob("checkpoint_*.pt"))
            if checkpoints:
                result.checkpoint_path = checkpoints[-1]
        
        return result
    
    def _run_simulated_training(self, work_dir: Path) -> TrainingResult:
        """
        æ‰§è¡Œæ¨¡æ‹Ÿè®­ç»ƒï¼ˆç”¨äºæµ‹è¯•æˆ– Mimic3 æœªå®‰è£…æ—¶ï¼‰
        
        Args:
            work_dir: å·¥ä½œç›®å½•
            
        Returns:
            è®­ç»ƒç»“æœ
        """
        result = TrainingResult()
        
        self._log("å¼€å§‹æ¨¡æ‹Ÿè®­ç»ƒ...")
        
        total_epochs = self.config.epochs
        steps_per_epoch = 100
        
        try:
            for epoch in range(1, total_epochs + 1):
                if self._should_stop:
                    self._update_progress(
                        status=TrainingStatus.CANCELLED,
                        message="è®­ç»ƒå·²å–æ¶ˆ"
                    )
                    result.error_message = "è®­ç»ƒè¢«ç”¨æˆ·å–æ¶ˆ"
                    return result
                
                for step in range(1, steps_per_epoch + 1):
                    if self._should_stop:
                        break
                    
                    # æ¨¡æ‹ŸæŸå¤±å€¼é€æ¸ä¸‹é™
                    simulated_loss = 1.0 / (epoch * 0.5 + step * 0.01 + 1)
                    
                    self._update_progress(
                        current_epoch=epoch,
                        total_epochs=total_epochs,
                        current_step=step,
                        total_steps=steps_per_epoch,
                        loss=simulated_loss,
                        learning_rate=0.0002 * (0.999875 ** (epoch * steps_per_epoch + step)),
                        status=TrainingStatus.TRAINING,
                        message=f"è®­ç»ƒä¸­: Epoch {epoch}/{total_epochs}, Step {step}/{steps_per_epoch}"
                    )
                    
                    # æ¨¡æ‹Ÿè®­ç»ƒæ—¶é—´
                    time.sleep(0.01)
                
                self._log(f"Epoch {epoch}/{total_epochs} å®Œæˆ, Loss: {self._progress.loss:.4f}")
            
            # åˆ›å»ºæ¨¡æ‹Ÿçš„è®­ç»ƒä¿¡æ¯æ–‡ä»¶ï¼ˆæ³¨æ„ï¼šè¿™ä¸æ˜¯æœ‰æ•ˆçš„ ONNX æ¨¡å‹ï¼ï¼‰
            model_dir = self.config.output_path / "model"
            model_dir.mkdir(parents=True, exist_ok=True)
            
            # ä½¿ç”¨ .json åç¼€ï¼Œæ˜ç¡®è¡¨æ˜è¿™ä¸æ˜¯ ONNX æ¨¡å‹
            info_path = model_dir / f"{self.config.speaker_name}_training_info.json"
            
            # å†™å…¥è®­ç»ƒä¿¡æ¯æ–‡ä»¶ï¼ˆä»…ç”¨äºè®°å½•ï¼Œä¸æ˜¯å¯ç”¨æ¨¡å‹ï¼‰
            model_info = {
                "speaker_name": self.config.speaker_name,
                "sample_rate": self.config.sample_rate,
                "epochs": self.config.epochs,
                "final_loss": self._progress.loss,
                "warning": "âš ï¸ è¿™æ˜¯æ¨¡æ‹Ÿè®­ç»ƒç”Ÿæˆçš„ä¿¡æ¯æ–‡ä»¶ï¼Œä¸æ˜¯æœ‰æ•ˆçš„ ONNX æ¨¡å‹ï¼",
                "solution": "è¯·å®‰è£… mimic3-train åé‡æ–°è®­ç»ƒä»¥ç”ŸæˆçœŸæ­£çš„ ONNX æ¨¡å‹: pip install mimic3-train"
            }
            with open(info_path, 'w', encoding='utf-8') as f:
                json.dump(model_info, f, indent=2, ensure_ascii=False)
            
            result.success = True
            result.model_path = info_path  # è¿”å›ä¿¡æ¯æ–‡ä»¶è·¯å¾„
            result.model_size = info_path.stat().st_size
            result.total_epochs = total_epochs
            result.final_loss = self._progress.loss
            result.training_time = time.time() - self._start_time
            
            self._update_progress(
                status=TrainingStatus.COMPLETED,
                message="æ¨¡æ‹Ÿè®­ç»ƒå®Œæˆï¼ˆæ³¨æ„ï¼šæœªç”Ÿæˆæœ‰æ•ˆçš„ ONNX æ¨¡å‹ï¼‰"
            )
            
            self._log("âš ï¸ è­¦å‘Š: æ¨¡æ‹Ÿè®­ç»ƒå·²å®Œæˆï¼Œä½†ç”±äº mimic3-train æœªå®‰è£…ï¼Œæ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„ ONNX æ¨¡å‹ï¼")
            self._log("ğŸ’¡ è§£å†³æ–¹æ¡ˆ: è¯·å®‰è£… mimic3-train åé‡æ–°è®­ç»ƒ: pip install mimic3-train")
            self._log(f"è®­ç»ƒä¿¡æ¯å·²ä¿å­˜è‡³: {info_path}")
            
        except Exception as e:
            result.success = False
            result.error_message = str(e)
            self._update_progress(
                status=TrainingStatus.FAILED,
                message=f"æ¨¡æ‹Ÿè®­ç»ƒå¤±è´¥: {e}"
            )
        
        return result
    
    def stop(self) -> None:
        """åœæ­¢è®­ç»ƒ"""
        self._should_stop = True
        self._log("æ”¶åˆ°åœæ­¢è®­ç»ƒè¯·æ±‚...")
        
        if self._process:
            self._log("ç»ˆæ­¢è®­ç»ƒè¿›ç¨‹...")
            self._process.terminate()
    
    def pause(self) -> Optional[Path]:
        """
        æš‚åœè®­ç»ƒå¹¶ä¿å­˜æ£€æŸ¥ç‚¹
        
        Returns:
            æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œå¦‚æœå¤±è´¥è¿”å› None
        """
        self._log("æš‚åœè®­ç»ƒ...")
        self._update_progress(
            status=TrainingStatus.PAUSED,
            message="è®­ç»ƒå·²æš‚åœ"
        )
        
        # ä¿å­˜å½“å‰çŠ¶æ€ä½œä¸ºæ£€æŸ¥ç‚¹
        checkpoint_dir = self.config.output_path / "training_workspace" / "checkpoints"
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        checkpoint_path = checkpoint_dir / f"checkpoint_epoch_{self._progress.current_epoch}.json"
        
        checkpoint_data = {
            "epoch": self._progress.current_epoch,
            "step": self._progress.current_step,
            "loss": self._progress.loss,
            "learning_rate": self._progress.learning_rate,
            "elapsed_time": self._progress.elapsed_time,
            "config": self.config.to_dict()
        }
        
        with open(checkpoint_path, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)
        
        self._log(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
        return checkpoint_path
    
    def get_progress(self) -> TrainingProgress:
        """
        è·å–å½“å‰è®­ç»ƒè¿›åº¦
        
        Returns:
            è®­ç»ƒè¿›åº¦å¯¹è±¡
        """
        return self._progress
    
    def get_logs(self) -> List[str]:
        """
        è·å–è®­ç»ƒæ—¥å¿—
        
        Returns:
            æ—¥å¿—åˆ—è¡¨
        """
        return self._logs.copy()


def create_trainer(config: TrainConfig, verbose: bool = False) -> Mimic3Trainer:
    """
    åˆ›å»ºè®­ç»ƒå™¨å®ä¾‹çš„å·¥å‚å‡½æ•°
    
    Args:
        config: è®­ç»ƒé…ç½®
        verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
        
    Returns:
        Mimic3Trainer å®ä¾‹
    """
    return Mimic3Trainer(config, verbose)
